m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
m <- nrow(X)
# номера «аномальных» наблюдений
anom.obs<-(1:m)[y==1]
la <- length(anom.obs)
m.cv <- round(0.2*(m-la)) + la
m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
# разделение выборки
X.train<-X[train.obs,]
X.cv <-X[cv.obs,]
y.train<-y[train.obs]
y.cv <-y[cv.obs]
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
m <- nrow(X)
# номера «аномальных» наблюдений
anom.obs<-(1:m)[y==1]
la <- length(anom.obs)
m.cv <- round(0.2*(m-la)) + la
m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
# разделение выборки
X.train <- X[train.obs,]
X.cv <- X[cv.obs,]
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
m <- nrow(X)
# номера «аномальных» наблюдений
anom.obs<-(1:m)[y==1]
la <- length(anom.obs)
m.cv <- round(0.2*(m-la)) + la
m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
# разделение выборки
X.train <- X[train.obs,]
X.cv <- X[cv.obs,]
y.train<-y[train.obs,]
y.cv <-y[cv.obs,]
# оценки параметров
mu <-apply(X.train, 2, mean)
sigma <-apply(X.train, 2, sd)
# функция «вероятности»
p <-function(X,mu,sigma) {
m <-nrow(X); n <-ncol(X)
prob<-matrix(nrow=m,ncol=n)
for (j in 1:n) prob[,j] <-dnorm(X[,j],mu[j],sigma[j])
apply(prob, 1, prod)
}
# определение «вероятностей»
prob.train<-p(X.train,mu,sigma)
prob.cv <-p(X.cv,mu,sigma)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps,fitStats(y.pred,y.cv)) )
}
dimnames(res)[[2]][1] <-"epsilon"# заголовки
# выбор наиболее подходящего 𝜀
j <-which.max(res[,"f1.score"])
eps<-res[j,"epsilon"]
# окончательный прогноз
y.pred<-1*(prob.cv < eps)
?fitStats
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps,fitstats(y.pred,y.cv)) )
}
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
}
?fitstats
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
fitstats
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
}
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
install.packages("metafor")
library(metafor)
for (eps in seq(pr[1], pr[2], length = 1000)) {
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
}
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
}
dimnames(res)[[2]][1] <-"epsilon"# заголовки
# выбор наиболее подходящего 𝜀
j <-which.max(res[,"f1.score"])
eps<-res[j,"epsilon"]
# окончательный прогноз
y.pred<-1*(prob.cv < eps)
plot(X)
plot(X$X, X$X.1)
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
m <- nrow(X)
# номера «аномальных» наблюдений
anom.obs<-(1:m)[y==1]
la <- length(anom.obs)
m.cv <- round(0.2*(m-la)) + la
m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
# разделение выборки
X.train <- X[train.obs,]
X.cv <- X[cv.obs,]
y.train<-y[train.obs,]
y.cv <-y[cv.obs,]
# оценки параметров
mu <-apply(X.train, 2, mean)
sigma <-apply(X.train, 2, sd)
# функция «вероятности»
p <-function(X,mu,sigma) {
m <-nrow(X); n <-ncol(X)
prob<-matrix(nrow=m,ncol=n)
for (j in 1:n) prob[,j] <-dnorm(X[,j],mu[j],sigma[j])
apply(prob, 1, prod)
}
# определение «вероятностей»
prob.train<-p(X.train,mu,sigma)
prob.cv <-p(X.cv,mu,sigma)
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
}
dimnames(res)[[2]][1] <-"epsilon"# заголовки
# выбор наиболее подходящего 𝜀
j <-which.max(res[,"f1.score"])
eps<-res[j,"epsilon"]
# окончательный прогноз
y.pred<-1*(prob.cv < eps)
?fitstats
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
}
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
X
?fitstats
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
m <- nrow(X)
# номера «аномальных» наблюдений
anom.obs<-(1:m)[y==1]
la <- length(anom.obs)
m.cv <- round(0.2*(m-la)) + la
m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
# разделение выборки
X.train <- X[train.obs,]
X.cv <- X[cv.obs,]
y.train<-y[train.obs,]
y.cv <-y[cv.obs,]
# оценки параметров
mu <-apply(X.train, 2, mean)
sigma <-apply(X.train, 2, sd)
# функция «вероятности»
p <-function(X,mu,sigma) {
m <-nrow(X); n <-ncol(X)
prob<-matrix(nrow=m,ncol=n)
for (j in 1:n) prob[,j] <-dnorm(X[,j],mu[j],sigma[j])
apply(prob, 1, prod)
}
# определение «вероятностей»
prob.train<-p(X.train,mu,sigma)
prob.cv <-p(X.cv,mu,sigma)
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
}
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
m <- nrow(X)
# номера «аномальных» наблюдений
anom.obs<-(1:m)[y==1]
la <- length(anom.obs)
m.cv <- round(0.2*(m-la)) + la
m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
# разделение выборки
X.train <- X[train.obs,]
X.cv <- X[cv.obs,]
y.train<-y[train.obs,]
y.cv <-y[cv.obs,]
# оценки параметров
mu <-apply(X.train, 2, mean)
sigma <-apply(X.train, 2, sd)
# функция «вероятности»
p <-function(X,mu,sigma) {
m <-nrow(X); n <-ncol(X)
prob<-matrix(nrow=m,ncol=n)
for (j in 1:n) prob[,j] <-dnorm(X[,j],mu[j],sigma[j])
apply(prob, 1, prod)
}
# определение «вероятностей»
prob.train<-p(X.train,mu,sigma)
prob.cv <-p(X.cv,mu,sigma)
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitStats(y.pred, y.cv)))
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
m <- nrow(X)
# номера «аномальных» наблюдений
anom.obs<-(1:m)[y==1]
la <- length(anom.obs)
m.cv <- round(0.2*(m-la)) + la
m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
# разделение выборки
X.train <- X[train.obs,]
X.cv <- X[cv.obs,]
y.train<-y[train.obs,]
y.cv <-y[cv.obs,]
# оценки параметров
mu <-apply(X.train, 2, mean)
sigma <-apply(X.train, 2, sd)
# функция «вероятности»
p <-function(X,mu,sigma) {
m <-nrow(X); n <-ncol(X)
prob<-matrix(nrow=m,ncol=n)
for (j in 1:n) prob[,j] <-dnorm(X[,j],mu[j],sigma[j])
apply(prob, 1, prod)
}
# определение «вероятностей»
prob.train<-p(X.train,mu,sigma)
prob.cv <-p(X.cv,mu,sigma)
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitStats(y.pred, y.cv)))
}
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
m <- nrow(X)
# номера «аномальных» наблюдений
anom.obs<-(1:m)[y==1]
la <- length(anom.obs)
m.cv <- round(0.2*(m-la)) + la
m.train<- m-m.cv
# номера экзаменующей и обучающей выборок
cv.obs <- c(sample((1:m)[-anom.obs], size=m.cv-la, replace=FALSE), anom.obs)
train.obs<-(1:m)[-cv.obs]
# разделение выборки
X.train <- X[train.obs,]
X.cv <- X[cv.obs,]
y.train<-y[train.obs,]
y.cv <-y[cv.obs,]
# оценки параметров
mu <-apply(X.train, 2, mean)
sigma <-apply(X.train, 2, sd)
# функция «вероятности»
p <-function(X,mu,sigma) {
m <-nrow(X); n <-ncol(X)
prob<-matrix(nrow=m,ncol=n)
for (j in 1:n) prob[,j] <-dnorm(X[,j],mu[j],sigma[j])
apply(prob, 1, prod)
}
# определение «вероятностей»
prob.train<-p(X.train,mu,sigma)
prob.cv <-p(X.cv,mu,sigma)
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstatistics(y.pred, y.cv)))
}
library(metafor)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitstats(y.pred, y.cv)))
}
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1, labels = y)
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
text(X$X,X$X.1,labels = y
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
text(X$X,X$X.1,labels = y)
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1, type = "n")
text(X$X,X$X.1,labels = y)
train <- read.csv('C:\\Users\\asedov001\\Documents\\CMF\\Anomaly Detection\\AD_comp_train.csv')
X <- train[c('X', 'X.1')]
y <- train['y']
plot(X$X, X$X.1)
?which
?where
y1 <- y[y$y!=0]
y1 <- y[y!=0]
y1
train
plot (train$X, train$X.1, col=ifelse(train$y=1, 'black', 'red'))
plot (train$X, train$X.1, col=ifelse(train$y<1, 'black', 'red'))
plot (train$X, train$X.1, col=ifelse(train$y<1, 'black', 'red'), pch = 19)
plot (train$X, train$X.1, col=ifelse(train$y<1, 'black', 'red'), pch = 10)
plot (train$X, train$X.1, col=ifelse(train$y<1, 'black', 'red'), pch = 15)
plot (train$X, train$X.1, col=ifelse(train$y<1, 'black', 'red'))
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
fitStats<-function(y,y.pred) {
...
if (precision + recall == 0) f1.score <-0
stat <-c(accuracy,precision,recall,f1.score)
names(stat) <-c("accuracy","precision","recall","f1.score")
stat
}
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitStats(y.pred, y.cv)))
}
dimnames(res)[[2]][1] <-"epsilon"# заголовки
# выбор наиболее подходящего 𝜀
j <-which.max(res[,"f1.score"])
eps<-res[j,"epsilon"]
# окончательный прогноз
y.pred<-1*(prob.cv < eps)
pr<-range(prob.cv) # границы возможных значений 𝜀
res <-NULL# в неё будут сохраняться результаты моделирования
# для каждого наблюдения экзаменующей выборки рассчитываем
# прогноз при определённом значении 𝜀и сравниваем его с фактом
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, eps)
}
dimnames(res)[[2]][1] <-"epsilon"# заголовки
# выбор наиболее подходящего 𝜀
j <-which.max(res[,"f1.score"])
eps<-res[j,"epsilon"]
# окончательный прогноз
y.pred<-1*(prob.cv < eps)
fitStats<-function(y,y.pred) {
precision <- posPredValue(y.pred, y)
recall <- sensitivity(y.pred, y)
f1.score <- (2 * precision * recall) / (precision + recall)
if (precision + recall == 0) f1.score <-0
stat <-c(precision,recall,f1.score)
names(stat) <-c(precision","recall","f1.score")
stat
}
fitStats<-function(y,y.pred) {
precision <- posPredValue(y.pred, y)
recall <- sensitivity(y.pred, y)
f1.score <- (2 * precision * recall) / (precision + recall)
if (precision + recall == 0) f1.score <-0
stat <-c(accuracy,precision,recall,f1.score)
names(stat) <-c("accuracy","precision","recall","f1.score")
stat
}
fitStats<-function(y,y.pred) {
precision <- posPredValue(y.pred, y)
recall <- sensitivity(y.pred, y)
f1.score <- (2 * precision * recall) / (precision + recall)
if (precision + recall == 0) f1.score <-0
stat <-c(precision,recall,f1.score)
names(stat) <-c("precision","recall","f1.score")
stat
}
for (eps in seq(pr[1], pr[2], length = 1000)) {
y.pred<-1*(prob.cv < eps)
res <-rbind(res, c(eps, fitStats(y.pred, y.cv)))
}
?posPredValue
library(caret)
fitStats<-function(y,y.pred) {
precision <- posPredValue(y.pred, y)
recall <- sensitivity(y.pred, y)
f1.score <- (2 * precision * recall) / (precision + recall)
if (precision + recall == 0) f1.score <-0
stat <-c(precision,recall,f1.score)
names(stat) <-c("precision","recall","f1.score")
stat
}
install.packages("caret")
library(caret)
fitStats<-function(y,y.pred) {
precision <- posPredValue(y.pred, y)
recall <- sensitivity(y.pred, y)
f1.score <- (2 * precision * recall) / (precision + recall)
if (precision + recall == 0) f1.score <-0
stat <-c(precision,recall,f1.score)
names(stat) <-c("precision","recall","f1.score")
stat
}
?sensitivity
install.packages("caretEnsemble")
library(caret)
fitStats<-function(y,y.pred) {
precision <- posPredValue(y.pred, y)
recall <- sensitivity(y.pred, y)
f1.score <- (2 * precision * recall) / (precision + recall)
if (precision + recall == 0) f1.score <-0
stat <-c(precision,recall,f1.score)
names(stat) <-c("precision","recall","f1.score")
stat
}
install.packages("caret")
library(caret)
install.packages("caret")
install.packages("caret")
install.packages("caret")
?install.packages
install.packages("caret")
library("caretEnsemble", lib.loc="~/R/win-library/3.3")
install.packages("caret")
